{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine_learning.ipynb",
      "provenance": [],
      "mount_file_id": "1b6wgMzITnjS6J3T8nB2rxGza9WVnQOBE",
      "authorship_tag": "ABX9TyN+/oudKnsqjqmnQv0IN5In",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoainv99/classify_comment/blob/master/ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px1LX_8ubNST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import lr_scheduler\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6DdhVXHbtgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxg5buaGbsSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hyper parameters\n",
        "input_size=100\n",
        "hidden_size=300\n",
        "output_size=2\n",
        "num_epochs=50\n",
        "learning_rate=0.001\n",
        "batch_size=16\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V6rmCLzbwKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=np.load('/content/drive/My Drive/best_data.npy')\n",
        "label=np.load('/content/drive/My Drive/best_label.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NDylO6HEwSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lb = []\n",
        "for i in label:\n",
        "  if i==0 :\n",
        "    lb.append([1,0])\n",
        "  else :\n",
        "    lb.append([0,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EjPYMGIh3gY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(data,lb,test_size=0.33,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOGLhjVPiRhf",
        "colab_type": "code",
        "outputId": "ef36c356-68ec-4b36-8129-4cc26b1606a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8StkNT30iFU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Custom_Dataset(Dataset):\n",
        "  def __init__(self, data, label):\n",
        "    self.data = data\n",
        "    self.label = label\n",
        "  def __getitem__(self, index):\n",
        "    X = torch.from_numpy(self.data[index]).to(device).float()\n",
        "    y = torch.from_numpy(np.array(self.label[index])).to(device).float()\n",
        "    return torch.squeeze(X),torch.squeeze(y) \n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUd6E6i7Bl-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = Custom_Dataset(x_train,y_train)\n",
        "val_set = Custom_Dataset(x_test,y_test)\n",
        "\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2SsMeBoW1Fj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x,y = next(iter(train_dataloader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5VbYcJ78-Ok",
        "colab_type": "code",
        "outputId": "07a89d2c-a8b2-4ad6-e542-0a1935c7bb53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x.shape,y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 100]), torch.Size([16, 2]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vuTbzVRbpXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetword(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,output_size):\n",
        "    super(NeuralNetword,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size,hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size,hidden_size)\n",
        "    self.fc3 = nn.Linear(hidden_size,output_size)\n",
        "\n",
        "  def forward(self,input):\n",
        "    out = self.fc1(input)\n",
        "    out = torch.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    out = torch.relu(out)\n",
        "    out = self.fc3(out)\n",
        "    out = torch.sigmoid(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFdobjO9boLC",
        "colab_type": "code",
        "outputId": "1761c995-1998-465a-9985-c86a9aa4dd88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "model=NeuralNetword(input_size,hidden_size,output_size)\n",
        "model.cuda()\n",
        "criterion=nn.BCELoss()\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)     "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-7dc383dabbc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'StepLR' object has no attribute 'StepLR'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hST1Vh7MLk-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Averager:\n",
        "    def __init__(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0\n",
        "\n",
        "    def send(self, value):\n",
        "        self.current_total += value\n",
        "        self.iterations += 1\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        if self.iterations == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1.0 * self.current_total / self.iterations\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zonHw90JOo1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r68BE9voblmU",
        "colab_type": "code",
        "outputId": "84844d4c-e343-4d1f-9427-a4b23e19822d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "loss_hist_train = Averager()\n",
        "loss_hist_val = Averager()\n",
        "for epoch in range(num_epochs):\n",
        "    loss_hist_train.reset()\n",
        "    total_train = total_val = 0\n",
        "    correct_train = correct_val = 0\n",
        "    for batch_idx,(images, labels) in enumerate(train_dataloader):\n",
        "       \n",
        "        outputs = model(images)\n",
        "        loss_train = criterion(outputs,labels)\n",
        "\n",
        "        loss_hist_train.send(loss_train)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Iteration #{batch_idx} loss: {loss_train.item()}\")\n",
        "        _,predicted = torch.max(outputs.data,1)\n",
        "        _,labels = torch.max(labels.data,1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train +=(predicted == labels).float().sum()\n",
        "    \n",
        "    for batch_idx,(images, labels) in enumerate(test_dataloader):   \n",
        "        outputs = model(images)\n",
        "\n",
        "        loss_val = criterion(outputs,labels)\n",
        "\n",
        "        loss_hist_val.send(loss_val)\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Iteration #{batch_idx} loss: {loss_val.item()}\")\n",
        "        _,predicted = torch.max(outputs.data,1)\n",
        "        _,labels = torch.max(labels.data,1)\n",
        "        total_val += labels.size(0)\n",
        "        correct_val +=(predicted == labels).float().sum()\n",
        "    lr_scheduler.step()\n",
        "    print(f\"epoch #{epoch} train_loss: {loss_hist_train.value} val_loss: {loss_hist_val.value} acc_train: {100* correct_train / total_train} acc_val:{100* correct_val / total_val }\")   \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration #0 loss: 0.6923997402191162\n",
            "Iteration #100 loss: 0.34957486391067505\n",
            "Iteration #200 loss: 0.24698050320148468\n",
            "Iteration #300 loss: 0.3581673502922058\n",
            "Iteration #400 loss: 0.29601508378982544\n",
            "Iteration #500 loss: 0.10175138711929321\n",
            "Iteration #600 loss: 0.29748088121414185\n",
            "Iteration #0 loss: 0.3314845561981201\n",
            "Iteration #100 loss: 0.18654516339302063\n",
            "Iteration #200 loss: 0.29709964990615845\n",
            "Iteration #300 loss: 0.3870135247707367\n",
            "epoch #0 train_loss: 0.36797937750816345 val_loss: 0.32788798213005066 acc_train: 83.825439453125 acc_val:85.63619232177734\n",
            "Iteration #0 loss: 0.34065449237823486\n",
            "Iteration #100 loss: 0.2831583619117737\n",
            "Iteration #200 loss: 0.16330185532569885\n",
            "Iteration #300 loss: 0.2930181622505188\n",
            "Iteration #400 loss: 0.18316048383712769\n",
            "Iteration #500 loss: 0.5234241485595703\n",
            "Iteration #600 loss: 0.15218569338321686\n",
            "Iteration #0 loss: 0.295199990272522\n",
            "Iteration #100 loss: 0.3697766661643982\n",
            "Iteration #200 loss: 0.6169006824493408\n",
            "Iteration #300 loss: 0.16859325766563416\n",
            "epoch #1 train_loss: 0.3335858881473541 val_loss: 0.32300013303756714 acc_train: 85.69173431396484 acc_val:86.25824737548828\n",
            "Iteration #0 loss: 0.1751931607723236\n",
            "Iteration #100 loss: 0.1993996948003769\n",
            "Iteration #200 loss: 0.4034532904624939\n",
            "Iteration #300 loss: 0.3996793031692505\n",
            "Iteration #400 loss: 0.17983882129192352\n",
            "Iteration #500 loss: 0.18966948986053467\n",
            "Iteration #600 loss: 0.21721741557121277\n",
            "Iteration #0 loss: 0.862310528755188\n",
            "Iteration #100 loss: 0.3029789328575134\n",
            "Iteration #200 loss: 0.1663537323474884\n",
            "Iteration #300 loss: 0.3304658532142639\n",
            "epoch #2 train_loss: 0.32234787940979004 val_loss: 0.3249939978122711 acc_train: 86.03528594970703 acc_val:86.39019775390625\n",
            "Iteration #0 loss: 0.3563538193702698\n",
            "Iteration #100 loss: 0.21254636347293854\n",
            "Iteration #200 loss: 0.13793721795082092\n",
            "Iteration #300 loss: 0.4244009256362915\n",
            "Iteration #400 loss: 0.412964403629303\n",
            "Iteration #500 loss: 0.5312142968177795\n",
            "Iteration #600 loss: 0.2971765100955963\n",
            "Iteration #0 loss: 0.20325997471809387\n",
            "Iteration #100 loss: 0.36597663164138794\n",
            "Iteration #200 loss: 0.33398139476776123\n",
            "Iteration #300 loss: 0.4178904592990875\n",
            "epoch #3 train_loss: 0.3151392340660095 val_loss: 0.3243480622768402 acc_train: 86.62952423095703 acc_val:86.14514923095703\n",
            "Iteration #0 loss: 0.2609959840774536\n",
            "Iteration #100 loss: 0.3625829219818115\n",
            "Iteration #200 loss: 0.07156779617071152\n",
            "Iteration #300 loss: 0.38187679648399353\n",
            "Iteration #400 loss: 0.2829104959964752\n",
            "Iteration #500 loss: 0.2932828962802887\n",
            "Iteration #600 loss: 0.2049138844013214\n",
            "Iteration #0 loss: 0.4655460715293884\n",
            "Iteration #100 loss: 0.207996666431427\n",
            "Iteration #200 loss: 0.44464176893234253\n",
            "Iteration #300 loss: 0.6973179578781128\n",
            "epoch #4 train_loss: 0.30792149901390076 val_loss: 0.3210074007511139 acc_train: 87.04735565185547 acc_val:86.71065521240234\n",
            "Iteration #0 loss: 0.4125359058380127\n",
            "Iteration #100 loss: 0.2556818723678589\n",
            "Iteration #200 loss: 0.18760524690151215\n",
            "Iteration #300 loss: 0.24981428682804108\n",
            "Iteration #400 loss: 0.32806333899497986\n",
            "Iteration #500 loss: 0.3883606195449829\n",
            "Iteration #600 loss: 0.1712852418422699\n",
            "Iteration #0 loss: 0.1935058832168579\n",
            "Iteration #100 loss: 0.3314468264579773\n",
            "Iteration #200 loss: 0.08772703260183334\n",
            "Iteration #300 loss: 0.15874844789505005\n",
            "epoch #5 train_loss: 0.3026728630065918 val_loss: 0.3198639154434204 acc_train: 87.42803955078125 acc_val:86.63525390625\n",
            "Iteration #0 loss: 0.22221775352954865\n",
            "Iteration #100 loss: 0.5181532502174377\n",
            "Iteration #200 loss: 0.181801438331604\n",
            "Iteration #300 loss: 0.11983775347471237\n",
            "Iteration #400 loss: 0.3058953881263733\n",
            "Iteration #500 loss: 0.3668186068534851\n",
            "Iteration #600 loss: 0.31868690252304077\n",
            "Iteration #0 loss: 0.23076753318309784\n",
            "Iteration #100 loss: 0.10663904249668121\n",
            "Iteration #200 loss: 0.6102472543716431\n",
            "Iteration #300 loss: 0.2068726122379303\n",
            "epoch #6 train_loss: 0.29539239406585693 val_loss: 0.3212861120700836 acc_train: 87.73444366455078 acc_val:85.84354400634766\n",
            "Iteration #0 loss: 0.6086281538009644\n",
            "Iteration #100 loss: 0.2686794400215149\n",
            "Iteration #200 loss: 0.5925590991973877\n",
            "Iteration #300 loss: 0.47527897357940674\n",
            "Iteration #400 loss: 0.258113294839859\n",
            "Iteration #500 loss: 0.07541237026453018\n",
            "Iteration #600 loss: 0.18758682906627655\n",
            "Iteration #0 loss: 0.08849866688251495\n",
            "Iteration #100 loss: 0.2427598088979721\n",
            "Iteration #200 loss: 0.25695621967315674\n",
            "Iteration #300 loss: 0.6008824706077576\n",
            "epoch #7 train_loss: 0.28462618589401245 val_loss: 0.32136550545692444 acc_train: 87.97586059570312 acc_val:86.95570373535156\n",
            "Iteration #0 loss: 0.5021276473999023\n",
            "Iteration #100 loss: 0.13518372178077698\n",
            "Iteration #200 loss: 0.3188636898994446\n",
            "Iteration #300 loss: 0.1805362105369568\n",
            "Iteration #400 loss: 0.18226689100265503\n",
            "Iteration #500 loss: 0.21727702021598816\n",
            "Iteration #600 loss: 0.23323631286621094\n",
            "Iteration #0 loss: 0.1399776041507721\n",
            "Iteration #100 loss: 0.27936288714408875\n",
            "Iteration #200 loss: 0.16384859383106232\n",
            "Iteration #300 loss: 0.24741458892822266\n",
            "epoch #8 train_loss: 0.2767065167427063 val_loss: 0.32050779461860657 acc_train: 88.46796417236328 acc_val:86.76720428466797\n",
            "Iteration #0 loss: 0.47617483139038086\n",
            "Iteration #100 loss: 0.2620790898799896\n",
            "Iteration #200 loss: 0.31430160999298096\n",
            "Iteration #300 loss: 0.4871375858783722\n",
            "Iteration #400 loss: 0.2504933178424835\n",
            "Iteration #500 loss: 0.40933603048324585\n",
            "Iteration #600 loss: 0.7894294261932373\n",
            "Iteration #0 loss: 0.09598305821418762\n",
            "Iteration #100 loss: 0.2875162363052368\n",
            "Iteration #200 loss: 0.31320273876190186\n",
            "Iteration #300 loss: 0.4321582317352295\n",
            "epoch #9 train_loss: 0.26861679553985596 val_loss: 0.32194840908050537 acc_train: 88.80223083496094 acc_val:86.65410614013672\n",
            "Iteration #0 loss: 0.26945173740386963\n",
            "Iteration #100 loss: 0.3739627003669739\n",
            "Iteration #200 loss: 0.05461013689637184\n",
            "Iteration #300 loss: 0.1582505702972412\n",
            "Iteration #400 loss: 0.2177705615758896\n",
            "Iteration #500 loss: 0.0731930211186409\n",
            "Iteration #600 loss: 0.4803401827812195\n",
            "Iteration #0 loss: 0.485729843378067\n",
            "Iteration #100 loss: 0.43596506118774414\n",
            "Iteration #200 loss: 0.17879840731620789\n",
            "Iteration #300 loss: 0.08556616306304932\n",
            "epoch #10 train_loss: 0.2631324827671051 val_loss: 0.32307323813438416 acc_train: 89.02507019042969 acc_val:85.46654510498047\n",
            "Iteration #0 loss: 0.22037547826766968\n",
            "Iteration #100 loss: 0.12175683677196503\n",
            "Iteration #200 loss: 0.12020807713270187\n",
            "Iteration #300 loss: 0.24168401956558228\n",
            "Iteration #400 loss: 0.35908254981040955\n",
            "Iteration #500 loss: 0.49909651279449463\n",
            "Iteration #600 loss: 0.4091131091117859\n",
            "Iteration #0 loss: 0.2751748859882355\n",
            "Iteration #100 loss: 0.20802275836467743\n",
            "Iteration #200 loss: 0.19431522488594055\n",
            "Iteration #300 loss: 0.3274327516555786\n",
            "epoch #11 train_loss: 0.25373563170433044 val_loss: 0.3238033950328827 acc_train: 89.37789916992188 acc_val:86.95570373535156\n",
            "Iteration #0 loss: 0.2518726885318756\n",
            "Iteration #100 loss: 0.04618994519114494\n",
            "Iteration #200 loss: 0.2624151408672333\n",
            "Iteration #300 loss: 0.2564079761505127\n",
            "Iteration #400 loss: 0.20214103162288666\n",
            "Iteration #500 loss: 0.28176477551460266\n",
            "Iteration #600 loss: 0.1247711107134819\n",
            "Iteration #0 loss: 0.2560693621635437\n",
            "Iteration #100 loss: 0.406529039144516\n",
            "Iteration #200 loss: 0.3858270049095154\n",
            "Iteration #300 loss: 0.3522876799106598\n",
            "epoch #12 train_loss: 0.2447919100522995 val_loss: 0.3242623209953308 acc_train: 89.75859069824219 acc_val:85.99434661865234\n",
            "Iteration #0 loss: 0.16628126800060272\n",
            "Iteration #100 loss: 0.13172924518585205\n",
            "Iteration #200 loss: 0.07208114862442017\n",
            "Iteration #300 loss: 0.15926899015903473\n",
            "Iteration #400 loss: 0.20920264720916748\n",
            "Iteration #500 loss: 0.2882702052593231\n",
            "Iteration #600 loss: 0.33696871995925903\n",
            "Iteration #0 loss: 0.3109402060508728\n",
            "Iteration #100 loss: 0.42353910207748413\n",
            "Iteration #200 loss: 0.3357236385345459\n",
            "Iteration #300 loss: 0.311886727809906\n",
            "epoch #13 train_loss: 0.23746389150619507 val_loss: 0.3247784972190857 acc_train: 90.25069427490234 acc_val:86.74835205078125\n",
            "Iteration #0 loss: 0.2053745687007904\n",
            "Iteration #100 loss: 0.15358051657676697\n",
            "Iteration #200 loss: 0.3564389646053314\n",
            "Iteration #300 loss: 0.16481702029705048\n",
            "Iteration #400 loss: 0.2902597486972809\n",
            "Iteration #500 loss: 0.21915589272975922\n",
            "Iteration #600 loss: 0.27526161074638367\n",
            "Iteration #0 loss: 0.0962400808930397\n",
            "Iteration #100 loss: 0.18869823217391968\n",
            "Iteration #200 loss: 0.5945529937744141\n",
            "Iteration #300 loss: 0.37476858496665955\n",
            "epoch #14 train_loss: 0.22898729145526886 val_loss: 0.3259853720664978 acc_train: 90.56639099121094 acc_val:86.277099609375\n",
            "Iteration #0 loss: 0.3278557062149048\n",
            "Iteration #100 loss: 0.031427886337041855\n",
            "Iteration #200 loss: 0.16598182916641235\n",
            "Iteration #300 loss: 0.15020759403705597\n",
            "Iteration #400 loss: 0.19916586577892303\n",
            "Iteration #500 loss: 0.06207922846078873\n",
            "Iteration #600 loss: 0.3207777142524719\n",
            "Iteration #0 loss: 0.1781558245420456\n",
            "Iteration #100 loss: 0.23410464823246002\n",
            "Iteration #200 loss: 0.26949232816696167\n",
            "Iteration #300 loss: 0.25335538387298584\n",
            "epoch #15 train_loss: 0.22253596782684326 val_loss: 0.3275691568851471 acc_train: 90.7613754272461 acc_val:86.05089569091797\n",
            "Iteration #0 loss: 0.6465215682983398\n",
            "Iteration #100 loss: 0.17993149161338806\n",
            "Iteration #200 loss: 0.1622934192419052\n",
            "Iteration #300 loss: 0.4186251759529114\n",
            "Iteration #400 loss: 0.21361176669597626\n",
            "Iteration #500 loss: 0.40162497758865356\n",
            "Iteration #600 loss: 0.44968581199645996\n",
            "Iteration #0 loss: 0.48768842220306396\n",
            "Iteration #100 loss: 0.3163105547428131\n",
            "Iteration #200 loss: 1.0099034309387207\n",
            "Iteration #300 loss: 0.2907412052154541\n",
            "epoch #16 train_loss: 0.21486711502075195 val_loss: 0.3291435241699219 acc_train: 91.04920959472656 acc_val:86.35250091552734\n",
            "Iteration #0 loss: 0.15253113210201263\n",
            "Iteration #100 loss: 0.33700814843177795\n",
            "Iteration #200 loss: 0.17006969451904297\n",
            "Iteration #300 loss: 0.14190678298473358\n",
            "Iteration #400 loss: 0.026310639455914497\n",
            "Iteration #500 loss: 0.2811134159564972\n",
            "Iteration #600 loss: 0.08441394567489624\n",
            "Iteration #0 loss: 0.47856542468070984\n",
            "Iteration #100 loss: 0.2586371898651123\n",
            "Iteration #200 loss: 0.17117497324943542\n",
            "Iteration #300 loss: 0.14835920929908752\n",
            "epoch #17 train_loss: 0.20580025017261505 val_loss: 0.3309767246246338 acc_train: 91.16991424560547 acc_val:85.97549438476562\n",
            "Iteration #0 loss: 0.0832754373550415\n",
            "Iteration #100 loss: 0.1039024144411087\n",
            "Iteration #200 loss: 0.07114218920469284\n",
            "Iteration #300 loss: 0.09266290068626404\n",
            "Iteration #400 loss: 0.09220752120018005\n",
            "Iteration #500 loss: 0.18497434258460999\n",
            "Iteration #600 loss: 0.27660778164863586\n",
            "Iteration #0 loss: 0.03102811984717846\n",
            "Iteration #100 loss: 0.08938263356685638\n",
            "Iteration #200 loss: 0.7792498469352722\n",
            "Iteration #300 loss: 0.13611559569835663\n",
            "epoch #18 train_loss: 0.19820575416088104 val_loss: 0.33348119258880615 acc_train: 91.84772491455078 acc_val:85.82469940185547\n",
            "Iteration #0 loss: 0.26341670751571655\n",
            "Iteration #100 loss: 0.22170619666576385\n",
            "Iteration #200 loss: 0.15029111504554749\n",
            "Iteration #300 loss: 0.2562256455421448\n",
            "Iteration #400 loss: 0.1907343715429306\n",
            "Iteration #500 loss: 0.20892232656478882\n",
            "Iteration #600 loss: 0.12107237428426743\n",
            "Iteration #0 loss: 0.32736682891845703\n",
            "Iteration #100 loss: 0.2582671642303467\n",
            "Iteration #200 loss: 0.3612385094165802\n",
            "Iteration #300 loss: 0.09729249775409698\n",
            "epoch #19 train_loss: 0.18948104977607727 val_loss: 0.3363320529460907 acc_train: 92.25626373291016 acc_val:85.1649398803711\n",
            "Iteration #0 loss: 0.33955883979797363\n",
            "Iteration #100 loss: 0.06259345263242722\n",
            "Iteration #200 loss: 0.1200338676571846\n",
            "Iteration #300 loss: 0.06161127984523773\n",
            "Iteration #400 loss: 0.2760617434978485\n",
            "Iteration #500 loss: 0.14013665914535522\n",
            "Iteration #600 loss: 0.07412586361169815\n",
            "Iteration #0 loss: 0.3525485098361969\n",
            "Iteration #100 loss: 0.6238309144973755\n",
            "Iteration #200 loss: 0.48105859756469727\n",
            "Iteration #300 loss: 0.2143000066280365\n",
            "epoch #20 train_loss: 0.18167728185653687 val_loss: 0.33968988060951233 acc_train: 92.38626098632812 acc_val:85.54194641113281\n",
            "Iteration #0 loss: 0.2092190682888031\n",
            "Iteration #100 loss: 0.25085556507110596\n",
            "Iteration #200 loss: 0.18311019241809845\n",
            "Iteration #300 loss: 0.1725262701511383\n",
            "Iteration #400 loss: 0.06205732002854347\n",
            "Iteration #500 loss: 0.04380618780851364\n",
            "Iteration #600 loss: 0.09245119988918304\n",
            "Iteration #0 loss: 0.5949749946594238\n",
            "Iteration #100 loss: 0.10232602059841156\n",
            "Iteration #200 loss: 0.2490478754043579\n",
            "Iteration #300 loss: 0.9453930854797363\n",
            "epoch #21 train_loss: 0.17413996160030365 val_loss: 0.3443733751773834 acc_train: 92.83193969726562 acc_val:85.65504455566406\n",
            "Iteration #0 loss: 0.30448684096336365\n",
            "Iteration #100 loss: 0.12910209596157074\n",
            "Iteration #200 loss: 0.16618287563323975\n",
            "Iteration #300 loss: 0.01797223649919033\n",
            "Iteration #400 loss: 0.2618425190448761\n",
            "Iteration #500 loss: 0.1300135999917984\n",
            "Iteration #600 loss: 0.03929683938622475\n",
            "Iteration #0 loss: 0.19526489078998566\n",
            "Iteration #100 loss: 0.04239346459507942\n",
            "Iteration #200 loss: 0.8519277572631836\n",
            "Iteration #300 loss: 0.13978053629398346\n",
            "epoch #22 train_loss: 0.16967904567718506 val_loss: 0.3485444188117981 acc_train: 92.8226547241211 acc_val:86.18284606933594\n",
            "Iteration #0 loss: 0.12566104531288147\n",
            "Iteration #100 loss: 0.41994404792785645\n",
            "Iteration #200 loss: 0.16123077273368835\n",
            "Iteration #300 loss: 0.04140767455101013\n",
            "Iteration #400 loss: 0.17206645011901855\n",
            "Iteration #500 loss: 0.05492246523499489\n",
            "Iteration #600 loss: 0.2025383859872818\n",
            "Iteration #0 loss: 0.23058946430683136\n",
            "Iteration #100 loss: 0.05892176926136017\n",
            "Iteration #200 loss: 0.9164708256721497\n",
            "Iteration #300 loss: 0.8268107175827026\n",
            "epoch #23 train_loss: 0.15661783516407013 val_loss: 0.3533552885055542 acc_train: 93.57474517822266 acc_val:85.80584716796875\n",
            "Iteration #0 loss: 0.18187172710895538\n",
            "Iteration #100 loss: 0.11389396339654922\n",
            "Iteration #200 loss: 0.04139550030231476\n",
            "Iteration #300 loss: 0.3401753604412079\n",
            "Iteration #400 loss: 0.06934760510921478\n",
            "Iteration #500 loss: 0.1265120804309845\n",
            "Iteration #600 loss: 0.10295575857162476\n",
            "Iteration #0 loss: 0.28223684430122375\n",
            "Iteration #100 loss: 0.4641273021697998\n",
            "Iteration #200 loss: 0.5052320957183838\n",
            "Iteration #300 loss: 0.7376993894577026\n",
            "epoch #24 train_loss: 0.14970923960208893 val_loss: 0.35801270604133606 acc_train: 94.02970886230469 acc_val:85.78699493408203\n",
            "Iteration #0 loss: 0.2490091174840927\n",
            "Iteration #100 loss: 0.2870084047317505\n",
            "Iteration #200 loss: 0.186062291264534\n",
            "Iteration #300 loss: 0.3759106397628784\n",
            "Iteration #400 loss: 0.04492698982357979\n",
            "Iteration #500 loss: 0.09205138683319092\n",
            "Iteration #600 loss: 0.21108534932136536\n",
            "Iteration #0 loss: 0.5164287090301514\n",
            "Iteration #100 loss: 0.05776049941778183\n",
            "Iteration #200 loss: 0.7191952466964722\n",
            "Iteration #300 loss: 1.5486655235290527\n",
            "epoch #25 train_loss: 0.143389493227005 val_loss: 0.36480221152305603 acc_train: 94.04827880859375 acc_val:85.3345947265625\n",
            "Iteration #0 loss: 0.24165239930152893\n",
            "Iteration #100 loss: 0.046242330223321915\n",
            "Iteration #200 loss: 0.1584467738866806\n",
            "Iteration #300 loss: 0.1445777714252472\n",
            "Iteration #400 loss: 0.02446657605469227\n",
            "Iteration #500 loss: 0.038770075887441635\n",
            "Iteration #600 loss: 0.46510523557662964\n",
            "Iteration #0 loss: 0.941328763961792\n",
            "Iteration #100 loss: 0.47224777936935425\n",
            "Iteration #200 loss: 0.20530113577842712\n",
            "Iteration #300 loss: 1.3442256450653076\n",
            "epoch #26 train_loss: 0.14114980399608612 val_loss: 0.36977842450141907 acc_train: 94.50325012207031 acc_val:84.05278015136719\n",
            "Iteration #0 loss: 0.10796365141868591\n",
            "Iteration #100 loss: 0.0682486966252327\n",
            "Iteration #200 loss: 0.08680443465709686\n",
            "Iteration #300 loss: 0.17597144842147827\n",
            "Iteration #400 loss: 0.009283438324928284\n",
            "Iteration #500 loss: 0.042868658900260925\n",
            "Iteration #600 loss: 0.19216492772102356\n",
            "Iteration #0 loss: 0.043426573276519775\n",
            "Iteration #100 loss: 1.0011792182922363\n",
            "Iteration #200 loss: 0.588038980960846\n",
            "Iteration #300 loss: 0.18158790469169617\n",
            "epoch #27 train_loss: 0.12763802707195282 val_loss: 0.37729352712631226 acc_train: 94.71680450439453 acc_val:86.05089569091797\n",
            "Iteration #0 loss: 0.135368213057518\n",
            "Iteration #100 loss: 0.07764983177185059\n",
            "Iteration #200 loss: 0.06319044530391693\n",
            "Iteration #300 loss: 0.003400038927793503\n",
            "Iteration #400 loss: 0.2134976089000702\n",
            "Iteration #500 loss: 0.03802355378866196\n",
            "Iteration #600 loss: 0.13161176443099976\n",
            "Iteration #0 loss: 0.5462659597396851\n",
            "Iteration #100 loss: 0.015356548130512238\n",
            "Iteration #200 loss: 0.968914806842804\n",
            "Iteration #300 loss: 0.18229517340660095\n",
            "epoch #28 train_loss: 0.12800660729408264 val_loss: 0.3859708607196808 acc_train: 94.99535369873047 acc_val:85.57964324951172\n",
            "Iteration #0 loss: 0.04873688519001007\n",
            "Iteration #100 loss: 0.05890285223722458\n",
            "Iteration #200 loss: 0.0034400273580104113\n",
            "Iteration #300 loss: 0.046394314616918564\n",
            "Iteration #400 loss: 0.1248316690325737\n",
            "Iteration #500 loss: 0.03541717305779457\n",
            "Iteration #600 loss: 0.17520922422409058\n",
            "Iteration #0 loss: 0.12168006598949432\n",
            "Iteration #100 loss: 0.6502602100372314\n",
            "Iteration #200 loss: 0.6048693060874939\n",
            "Iteration #300 loss: 0.3893428444862366\n",
            "epoch #29 train_loss: 0.12079958617687225 val_loss: 0.3946823477745056 acc_train: 95.18106079101562 acc_val:85.5230941772461\n",
            "Iteration #0 loss: 0.0019602268002927303\n",
            "Iteration #100 loss: 0.011864090338349342\n",
            "Iteration #200 loss: 0.24965253472328186\n",
            "Iteration #300 loss: 0.2889975607395172\n",
            "Iteration #400 loss: 0.040243200957775116\n",
            "Iteration #500 loss: 0.28160518407821655\n",
            "Iteration #600 loss: 0.12381096184253693\n",
            "Iteration #0 loss: 0.30114656686782837\n",
            "Iteration #100 loss: 0.20596694946289062\n",
            "Iteration #200 loss: 0.49919912219047546\n",
            "Iteration #300 loss: 0.16672155261039734\n",
            "epoch #30 train_loss: 0.11071330308914185 val_loss: 0.4018124043941498 acc_train: 95.45960998535156 acc_val:85.3345947265625\n",
            "Iteration #0 loss: 0.15694063901901245\n",
            "Iteration #100 loss: 0.0464460663497448\n",
            "Iteration #200 loss: 0.1531461775302887\n",
            "Iteration #300 loss: 0.1165006011724472\n",
            "Iteration #400 loss: 0.24317854642868042\n",
            "Iteration #500 loss: 0.07985779643058777\n",
            "Iteration #600 loss: 0.5995761752128601\n",
            "Iteration #0 loss: 0.21589860320091248\n",
            "Iteration #100 loss: 0.7370310425758362\n",
            "Iteration #200 loss: 0.28608468174934387\n",
            "Iteration #300 loss: 1.7136906385421753\n",
            "epoch #31 train_loss: 0.10584603250026703 val_loss: 0.40953049063682556 acc_train: 95.89600372314453 acc_val:85.25919342041016\n",
            "Iteration #0 loss: 0.15829423069953918\n",
            "Iteration #100 loss: 0.07104575634002686\n",
            "Iteration #200 loss: 0.03681373596191406\n",
            "Iteration #300 loss: 0.01755334623157978\n",
            "Iteration #400 loss: 0.07909724861383438\n",
            "Iteration #500 loss: 0.09969733655452728\n",
            "Iteration #600 loss: 0.06268284469842911\n",
            "Iteration #0 loss: 1.5142674446105957\n",
            "Iteration #100 loss: 1.033349871635437\n",
            "Iteration #200 loss: 0.7271798849105835\n",
            "Iteration #300 loss: 0.6955780982971191\n",
            "epoch #32 train_loss: 0.10034142434597015 val_loss: 0.41879525780677795 acc_train: 96.10028076171875 acc_val:85.1649398803711\n",
            "Iteration #0 loss: 0.0710759237408638\n",
            "Iteration #100 loss: 0.07837080955505371\n",
            "Iteration #200 loss: 0.06790612637996674\n",
            "Iteration #300 loss: 0.004761524498462677\n",
            "Iteration #400 loss: 0.11953836679458618\n",
            "Iteration #500 loss: 0.1310565173625946\n",
            "Iteration #600 loss: 0.07115267962217331\n",
            "Iteration #0 loss: 1.2111167907714844\n",
            "Iteration #100 loss: 0.6598705053329468\n",
            "Iteration #200 loss: 1.2031116485595703\n",
            "Iteration #300 loss: 1.4347350597381592\n",
            "epoch #33 train_loss: 0.09492044895887375 val_loss: 0.42955854535102844 acc_train: 96.38811492919922 acc_val:85.57964324951172\n",
            "Iteration #0 loss: 0.0710332840681076\n",
            "Iteration #100 loss: 0.07559634745121002\n",
            "Iteration #200 loss: 0.17244169116020203\n",
            "Iteration #300 loss: 0.1722760647535324\n",
            "Iteration #400 loss: 0.02782689779996872\n",
            "Iteration #500 loss: 0.05634966492652893\n",
            "Iteration #600 loss: 0.22501571476459503\n",
            "Iteration #0 loss: 0.4713292121887207\n",
            "Iteration #100 loss: 0.9966363310813904\n",
            "Iteration #200 loss: 1.3569989204406738\n",
            "Iteration #300 loss: 1.1527247428894043\n",
            "epoch #34 train_loss: 0.09379998594522476 val_loss: 0.4421622157096863 acc_train: 96.30455017089844 acc_val:84.59943389892578\n",
            "Iteration #0 loss: 0.0995802953839302\n",
            "Iteration #100 loss: 0.29460975527763367\n",
            "Iteration #200 loss: 0.13654190301895142\n",
            "Iteration #300 loss: 0.21859218180179596\n",
            "Iteration #400 loss: 0.040609829127788544\n",
            "Iteration #500 loss: 0.07334750890731812\n",
            "Iteration #600 loss: 0.04932224750518799\n",
            "Iteration #0 loss: 0.14139094948768616\n",
            "Iteration #100 loss: 0.15922634303569794\n",
            "Iteration #200 loss: 0.13544902205467224\n",
            "Iteration #300 loss: 0.07156235724687576\n",
            "epoch #35 train_loss: 0.08640831708908081 val_loss: 0.45434337854385376 acc_train: 96.63880920410156 acc_val:85.71159362792969\n",
            "Iteration #0 loss: 0.060046158730983734\n",
            "Iteration #100 loss: 0.053821343928575516\n",
            "Iteration #200 loss: 0.19681595265865326\n",
            "Iteration #300 loss: 0.01031540147960186\n",
            "Iteration #400 loss: 0.08628058433532715\n",
            "Iteration #500 loss: 0.025852855294942856\n",
            "Iteration #600 loss: 0.06819705665111542\n",
            "Iteration #0 loss: 0.378487765789032\n",
            "Iteration #100 loss: 0.011424997821450233\n",
            "Iteration #200 loss: 0.1993437558412552\n",
            "Iteration #300 loss: 0.6583105325698853\n",
            "epoch #36 train_loss: 0.09583203494548798 val_loss: 0.46534866094589233 acc_train: 96.33240509033203 acc_val:83.84542846679688\n",
            "Iteration #0 loss: 0.0723467692732811\n",
            "Iteration #100 loss: 0.1983034908771515\n",
            "Iteration #200 loss: 0.22540000081062317\n",
            "Iteration #300 loss: 0.069865383207798\n",
            "Iteration #400 loss: 0.41282960772514343\n",
            "Iteration #500 loss: 0.0920414924621582\n",
            "Iteration #600 loss: 0.052779898047447205\n",
            "Iteration #0 loss: 0.6124749183654785\n",
            "Iteration #100 loss: 1.1678624153137207\n",
            "Iteration #200 loss: 1.8661826848983765\n",
            "Iteration #300 loss: 4.0439581871032715\n",
            "epoch #37 train_loss: 0.08360689133405685 val_loss: 0.4768107831478119 acc_train: 96.80593872070312 acc_val:84.31668853759766\n",
            "Iteration #0 loss: 0.04981284588575363\n",
            "Iteration #100 loss: 0.1168222427368164\n",
            "Iteration #200 loss: 0.07521751523017883\n",
            "Iteration #300 loss: 0.020478514954447746\n",
            "Iteration #400 loss: 0.03620529919862747\n",
            "Iteration #500 loss: 0.11725881695747375\n",
            "Iteration #600 loss: 0.045224629342556\n",
            "Iteration #0 loss: 0.8852627873420715\n",
            "Iteration #100 loss: 1.0264836549758911\n",
            "Iteration #200 loss: 1.8339006900787354\n",
            "Iteration #300 loss: 1.4510159492492676\n",
            "epoch #38 train_loss: 0.08322466164827347 val_loss: 0.4878171384334564 acc_train: 96.94522094726562 acc_val:85.3722915649414\n",
            "Iteration #0 loss: 0.0378086157143116\n",
            "Iteration #100 loss: 0.02078775316476822\n",
            "Iteration #200 loss: 0.11326819658279419\n",
            "Iteration #300 loss: 0.010512200184166431\n",
            "Iteration #400 loss: 0.1970396637916565\n",
            "Iteration #500 loss: 0.0034444271586835384\n",
            "Iteration #600 loss: 0.1302601844072342\n",
            "Iteration #0 loss: 0.6565015912055969\n",
            "Iteration #100 loss: 0.26332128047943115\n",
            "Iteration #200 loss: 0.11224335432052612\n",
            "Iteration #300 loss: 0.1698644608259201\n",
            "epoch #39 train_loss: 0.07262613624334335 val_loss: 0.49977487325668335 acc_train: 97.27019500732422 acc_val:85.22148895263672\n",
            "Iteration #0 loss: 0.09066931158304214\n",
            "Iteration #100 loss: 0.03989695757627487\n",
            "Iteration #200 loss: 0.2580161392688751\n",
            "Iteration #300 loss: 0.04554542899131775\n",
            "Iteration #400 loss: 0.009740365669131279\n",
            "Iteration #500 loss: 0.1653168499469757\n",
            "Iteration #600 loss: 0.006947876885533333\n",
            "Iteration #0 loss: 1.0349425077438354\n",
            "Iteration #100 loss: 0.7253352999687195\n",
            "Iteration #200 loss: 0.2994708716869354\n",
            "Iteration #300 loss: 0.6697113513946533\n",
            "epoch #40 train_loss: 0.0731649100780487 val_loss: 0.5107020735740662 acc_train: 97.2330551147461 acc_val:84.37323760986328\n",
            "Iteration #0 loss: 0.04776487499475479\n",
            "Iteration #100 loss: 0.26332998275756836\n",
            "Iteration #200 loss: 0.017381172627210617\n",
            "Iteration #300 loss: 0.08459892123937607\n",
            "Iteration #400 loss: 0.06064708158373833\n",
            "Iteration #500 loss: 0.02961181290447712\n",
            "Iteration #600 loss: 0.09623351693153381\n",
            "Iteration #0 loss: 1.115506649017334\n",
            "Iteration #100 loss: 0.8721539974212646\n",
            "Iteration #200 loss: 0.6595890522003174\n",
            "Iteration #300 loss: 0.23040160536766052\n",
            "epoch #41 train_loss: 0.07586222141981125 val_loss: 0.521059513092041 acc_train: 97.13092041015625 acc_val:85.22148895263672\n",
            "Iteration #0 loss: 0.022901996970176697\n",
            "Iteration #100 loss: 0.004753834567964077\n",
            "Iteration #200 loss: 0.04350265860557556\n",
            "Iteration #300 loss: 0.00930261705070734\n",
            "Iteration #400 loss: 0.005904537625610828\n",
            "Iteration #500 loss: 0.035625070333480835\n",
            "Iteration #600 loss: 0.24266521632671356\n",
            "Iteration #0 loss: 0.40607672929763794\n",
            "Iteration #100 loss: 1.4357693195343018\n",
            "Iteration #200 loss: 0.29634982347488403\n",
            "Iteration #300 loss: 0.2125302255153656\n",
            "epoch #42 train_loss: 0.06437048316001892 val_loss: 0.5341070294380188 acc_train: 97.6230239868164 acc_val:84.2224349975586\n",
            "Iteration #0 loss: 0.006848649121820927\n",
            "Iteration #100 loss: 0.06597314029932022\n",
            "Iteration #200 loss: 0.024051915854215622\n",
            "Iteration #300 loss: 0.025174463167786598\n",
            "Iteration #400 loss: 0.11345571279525757\n",
            "Iteration #500 loss: 0.16209736466407776\n",
            "Iteration #600 loss: 0.039850663393735886\n",
            "Iteration #0 loss: 4.209448337554932\n",
            "Iteration #100 loss: 0.9221689105033875\n",
            "Iteration #200 loss: 4.599437236785889\n",
            "Iteration #300 loss: 0.9443251490592957\n",
            "epoch #43 train_loss: 0.0669996589422226 val_loss: 0.546287477016449 acc_train: 97.48374938964844 acc_val:85.22148895263672\n",
            "Iteration #0 loss: 0.05202654004096985\n",
            "Iteration #100 loss: 0.01411215215921402\n",
            "Iteration #200 loss: 0.3233215808868408\n",
            "Iteration #300 loss: 0.03752443566918373\n",
            "Iteration #400 loss: 0.34548717737197876\n",
            "Iteration #500 loss: 0.023022599518299103\n",
            "Iteration #600 loss: 0.08546175062656403\n",
            "Iteration #0 loss: 1.7409849166870117\n",
            "Iteration #100 loss: 1.0848400592803955\n",
            "Iteration #200 loss: 0.43333113193511963\n",
            "Iteration #300 loss: 0.04953271523118019\n",
            "epoch #44 train_loss: 0.07312361150979996 val_loss: 0.5594967007637024 acc_train: 97.34447479248047 acc_val:85.31574249267578\n",
            "Iteration #0 loss: 0.10119213163852692\n",
            "Iteration #100 loss: 0.08050496131181717\n",
            "Iteration #200 loss: 0.11022333800792694\n",
            "Iteration #300 loss: 0.014018461108207703\n",
            "Iteration #400 loss: 0.0006463872850872576\n",
            "Iteration #500 loss: 0.0001903658703668043\n",
            "Iteration #600 loss: 0.10831249505281448\n",
            "Iteration #0 loss: 1.9060834646224976\n",
            "Iteration #100 loss: 0.09413653612136841\n",
            "Iteration #200 loss: 0.6020500659942627\n",
            "Iteration #300 loss: 0.560302197933197\n",
            "epoch #45 train_loss: 0.05763835459947586 val_loss: 0.5708919167518616 acc_train: 98.00371551513672 acc_val:84.9764404296875\n",
            "Iteration #0 loss: 0.0003128699027001858\n",
            "Iteration #100 loss: 0.00876754429191351\n",
            "Iteration #200 loss: 0.014520341530442238\n",
            "Iteration #300 loss: 0.033514078706502914\n",
            "Iteration #400 loss: 0.007240004371851683\n",
            "Iteration #500 loss: 0.09719575941562653\n",
            "Iteration #600 loss: 0.0024408698081970215\n",
            "Iteration #0 loss: 0.13805803656578064\n",
            "Iteration #100 loss: 0.032895687967538834\n",
            "Iteration #200 loss: 0.9456337690353394\n",
            "Iteration #300 loss: 0.26385316252708435\n",
            "epoch #46 train_loss: 0.059124916791915894 val_loss: 0.5854045748710632 acc_train: 97.71587371826172 acc_val:85.39114379882812\n",
            "Iteration #0 loss: 0.047886114567518234\n",
            "Iteration #100 loss: 0.043987032026052475\n",
            "Iteration #200 loss: 0.009008096531033516\n",
            "Iteration #300 loss: 0.027896378189325333\n",
            "Iteration #400 loss: 0.10757452994585037\n",
            "Iteration #500 loss: 0.13600298762321472\n",
            "Iteration #600 loss: 0.1154383197426796\n",
            "Iteration #0 loss: 0.3030111789703369\n",
            "Iteration #100 loss: 0.6051139831542969\n",
            "Iteration #200 loss: 0.37412992119789124\n",
            "Iteration #300 loss: 1.5378938913345337\n",
            "epoch #47 train_loss: 0.061055734753608704 val_loss: 0.5980520248413086 acc_train: 97.78087615966797 acc_val:84.05278015136719\n",
            "Iteration #0 loss: 0.8465755581855774\n",
            "Iteration #100 loss: 0.025031745433807373\n",
            "Iteration #200 loss: 0.0044561391696333885\n",
            "Iteration #300 loss: 0.14064374566078186\n",
            "Iteration #400 loss: 0.0018605960067361593\n",
            "Iteration #500 loss: 0.029529493302106857\n",
            "Iteration #600 loss: 0.024515733122825623\n",
            "Iteration #0 loss: 0.792870044708252\n",
            "Iteration #100 loss: 0.19101035594940186\n",
            "Iteration #200 loss: 1.0180587768554688\n",
            "Iteration #300 loss: 0.0958690270781517\n",
            "epoch #48 train_loss: 0.06038893759250641 val_loss: 0.6093150973320007 acc_train: 97.85515594482422 acc_val:84.7879409790039\n",
            "Iteration #0 loss: 0.015531061217188835\n",
            "Iteration #100 loss: 4.7767192882020026e-05\n",
            "Iteration #200 loss: 0.15020598471164703\n",
            "Iteration #300 loss: 0.05728839337825775\n",
            "Iteration #400 loss: 0.04360266029834747\n",
            "Iteration #500 loss: 0.009374362416565418\n",
            "Iteration #600 loss: 0.10574294626712799\n",
            "Iteration #0 loss: 0.48788338899612427\n",
            "Iteration #100 loss: 0.06333067268133163\n",
            "Iteration #200 loss: 0.3979470729827881\n",
            "Iteration #300 loss: 0.13723421096801758\n",
            "epoch #49 train_loss: 0.05817750468850136 val_loss: 0.6300936937332153 acc_train: 97.9108657836914 acc_val:85.50424194335938\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}